#!/bin/bash

# ä»£ç†é…ç½®å‡½æ•°
setup_proxy() {
    echo "é…ç½®ä»£ç†..." | tee -a $LOG_FILE
    
    # å¯åŠ¨ä»£ç†
    clashon
    sleep 2
    
    # è®¾ç½®å…¨å±€æ¨¡å¼
    curl -X PATCH http://127.0.0.1:9090/configs -d '{"mode": "Global"}' 2>/dev/null
    sleep 1
    
    # è·å–å¯ç”¨èŠ‚ç‚¹
    echo "è·å–å¯ç”¨ä»£ç†èŠ‚ç‚¹..." | tee -a $LOG_FILE
    available_proxies=$(curl -s http://127.0.0.1:9090/proxies/GLOBAL | python3 -c "
import json, sys
data = json.load(sys.stdin)
proxies = data.get('all', [])
for proxy in proxies[:5]:  # æ˜¾ç¤ºå‰5ä¸ªèŠ‚ç‚¹
    print(f'å¯ç”¨èŠ‚ç‚¹: {proxy}')
# ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
if proxies:
    print(f'ä½¿ç”¨èŠ‚ç‚¹: {proxies[0]}')
    sys.exit(0)
else:
    sys.exit(1)
")
    
    # è®¾ç½®ä»£ç†èŠ‚ç‚¹ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹ï¼‰
    first_proxy=$(curl -s http://127.0.0.1:9090/proxies/GLOBAL | python3 -c "
import json, sys
data = json.load(sys.stdin)
proxies = data.get('all', [])
if proxies:
    print(proxies[0])
")
    
    if [ ! -z "$first_proxy" ]; then
        curl -X PUT http://127.0.0.1:9090/proxies/GLOBAL -d "{\"name\": \"$first_proxy\"}" 2>/dev/null
        echo "ä»£ç†è®¾ç½®å®Œæˆ: $first_proxy" | tee -a $LOG_FILE
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        export http_proxy=http://127.0.0.1:7890
        export https_proxy=http://127.0.0.1:7890
        export HTTP_PROXY=http://127.0.0.1:7890
        export HTTPS_PROXY=http://127.0.0.1:7890
    else
        echo "è­¦å‘Š: æ— æ³•è·å–ä»£ç†èŠ‚ç‚¹ï¼Œç»§ç»­ä½¿ç”¨ç›´è¿" | tee -a $LOG_FILE
    fi
}

# æ£€æŸ¥ç½‘ç»œè¿æ¥
check_network() {
    echo "æ£€æŸ¥ç½‘ç»œè¿æ¥..." | tee -a $LOG_FILE
    if ! curl -s --connect-timeout 10 https://www.google.com > /dev/null; then
        echo "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œå°è¯•é…ç½®ä»£ç†..." | tee -a $LOG_FILE
        setup_proxy
        
        # å†æ¬¡æ£€æŸ¥
        if ! curl -s --connect-timeout 10 https://www.google.com > /dev/null; then
            echo "ä»£ç†é…ç½®åä»æ— æ³•è¿æ¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®" | tee -a $LOG_FILE
        else
            echo "ä»£ç†é…ç½®æˆåŠŸ" | tee -a $LOG_FILE
        fi
    else
        echo "ç½‘ç»œè¿æ¥æ­£å¸¸" | tee -a $LOG_FILE
    fi
}

# è®¾ç½®æ—¥å¿—æ–‡ä»¶
LOG_FILE="benchmark_mobile_test.log"
echo "å¼€å§‹æµ‹è¯• $(date)" > $LOG_FILE

# è®¾ç½®å·¥ä½œç›®å½•
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$SCRIPT_DIR"

# æ£€æŸ¥å¹¶é…ç½®ç½‘ç»œ
check_network

# äº¤äº’å¼é€‰æ‹©æ¨¡å‹
select_model() {
    echo "è¯·é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹:"
    echo "1) google/gemini-2.5-flash"
    echo "2) google/gemini-2.5-pro"
    echo "3) anthropic/claude-3.5-sonnet"
    echo "4) openai/gpt-4o-2024-11-20"
    echo "5) é€€å‡º"
    
    while true; do
        read -p "è¯·è¾“å…¥é€‰æ‹© (1-5): " choice
        case $choice in
            1)
                SELECTED_MODEL="google/gemini-2.5-flash"
                echo "âœ… å·²é€‰æ‹©: $SELECTED_MODEL"
                break
                ;;
            2)
                SELECTED_MODEL="google/gemini-2.5-pro"
                echo "âœ… å·²é€‰æ‹©: $SELECTED_MODEL"
                break
                ;;
            3)
                SELECTED_MODEL="anthropic/claude-3.5-sonnet"
                echo "âœ… å·²é€‰æ‹©: $SELECTED_MODEL"
                break
                ;;
            4)
                SELECTED_MODEL="openai/gpt-4o-2024-11-20"
                echo "âœ… å·²é€‰æ‹©: $SELECTED_MODEL"
                break
                ;;
            5)
                echo "é€€å‡ºè„šæœ¬..."
                exit 0
                ;;
            *)
                echo "âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-5"
                ;;
        esac
    done
}

# é€‰æ‹©æ¨¡å‹
select_model

# å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹
MODELS=("$SELECTED_MODEL")

# æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è®¾ç½®è¿›åº¦æ–‡ä»¶
MODEL_SAFE_NAME=$(echo "$SELECTED_MODEL" | sed 's/[^a-zA-Z0-9]/_/g')
PROGRESS_FILE="evaluation_progress_mobile_${MODEL_SAFE_NAME}.json"

echo "å½“å‰æ¨¡å‹: $SELECTED_MODEL" | tee -a $LOG_FILE
echo "è¿›åº¦æ–‡ä»¶: $PROGRESS_FILE" | tee -a $LOG_FILE

if [ -f "$PROGRESS_FILE" ]; then
    echo "å‘ç°è¯¥æ¨¡å‹çš„è¿›åº¦æ–‡ä»¶ï¼Œå°†ä»æ–­ç‚¹ç»§ç»­è¿è¡Œ..." | tee -a $LOG_FILE
else
    echo "æœªæ‰¾åˆ°è¯¥æ¨¡å‹çš„è¿›åº¦æ–‡ä»¶ï¼Œå°†å¼€å§‹å…¨æ–°è¯„ä¼°..." | tee -a $LOG_FILE
fi

# è¿è¡ŒPythonè„šæœ¬è¿›è¡Œè¯„ä¼°
echo "å¼€å§‹è¿è¡Œåœ¨çº¿æ¨¡å‹è¯„ä¼°..." | tee -a $LOG_FILE

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH="$SCRIPT_DIR"
export TEST_NON_INTERACTIVE=1
export SELECTED_MODEL="$SELECTED_MODEL"
export PROGRESS_FILE="$PROGRESS_FILE"

# åˆ›å»ºPythonè¯„ä¼°è„šæœ¬
cat > run_online_evaluation.py << 'EOF'
#!/usr/bin/env python3
import os
import sys
import logging
import time
import json
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.getcwd())

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è¿›åº¦æ–‡ä»¶è·¯å¾„ - å°†ç”±shellè„šæœ¬ä¼ å…¥
PROGRESS_FILE = os.environ.get('PROGRESS_FILE', 'evaluation_progress.json')

def load_progress():
    """åŠ è½½è¿›åº¦æ–‡ä»¶"""
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    return {}

def save_progress(progress_data):
    """ä¿å­˜è¿›åº¦åˆ°æ–‡ä»¶"""
    try:
        with open(PROGRESS_FILE, 'w') as f:
            json.dump(progress_data, f, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")

def is_task_completed(model_name, scenario, progress_data):
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ"""
    task_key = f"{model_name}_scenario_{scenario}"
    return progress_data.get(task_key, {}).get('completed', False)

def mark_task_completed(model_name, scenario, progress_data):
    """æ ‡è®°ä»»åŠ¡ä¸ºå·²å®Œæˆ"""
    task_key = f"{model_name}_scenario_{scenario}"
    progress_data[task_key] = {
        'completed': True,
        'completion_time': datetime.now().isoformat(),
        'model': model_name,
        'scenario': scenario
    }
    save_progress(progress_data)

def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("GPUå†…å­˜å·²æ¸…ç†")
    except Exception as e:
        logger.warning(f"æ¸…ç†GPUå†…å­˜å¤±è´¥: {e}")

def run_scenario_test(model_name: str, scenario: int, limit: int = 650) -> bool:
    """è¿è¡ŒæŒ‡å®šåœºæ™¯çš„æµ‹è¯•"""
    scenario_configs = {
        1: {
            "name": "åŸºå‡†æµ‹è¯•",
            "description": "ç›´æ¥æ¨¡å‹è¯„ä¼°ï¼Œæ— å¢å¼º",
            "detector_model": None,
            "use_ground_truth": False,
            "use_cache": False,
            "cache_source_dir": None
        },
        2: {
            "name": "ç»„ä»¶æ£€æµ‹å¢å¼º(GPT-4o)", 
            "description": "ä½¿ç”¨GPT-4oè¿›è¡Œç»„ä»¶æ£€æµ‹çš„å¢å¼ºprompt",
            "detector_model": "openai/gpt-4o-2024-11-20",
            "use_ground_truth": False,
            "use_cache": True,
            "cache_source_dir": "openai/gpt-4o-2024-11-20_bygpt-4o-2024-11-20"
        },
        3: {
            "name": "å®Œæ•´å¢å¼º",
            "description": "ä½¿ç”¨ç»„ä»¶æ£€æµ‹å™¨å’ŒçœŸå®è¾¹æ¡†",
            "detector_model": "google/gemini-2.5-flash", 
            "use_ground_truth": True,
            "use_cache": True,
            "cache_source_dir": "google/gemini-2.5-flash_bygemini-2.5-flash_groundtruth"
        }
    }
    
    scenario_info = scenario_configs[scenario]
    
    logger.info("="*60)
    logger.info(f"ğŸš€ å¼€å§‹æµ‹è¯•")
    logger.info(f"æ¨¡å‹: {model_name}")
    logger.info(f"åœºæ™¯: {scenario} ({scenario_info['name']})")
    logger.info(f"æè¿°: {scenario_info['description']}")
    logger.info(f"æµ‹è¯•ç”¨ä¾‹æ•°: {limit}")
    logger.info("="*60)
    
    # ç®€å•è¿›åº¦å›è°ƒ
    def simple_progress(current: int, total: int, message: str = ""):
        percentage = (current / total) * 100 if total > 0 else 0
        logger.info(f"è¿›åº¦: {current}/{total} ({percentage:.1f}%) - {message}")
    
    try:
        # å¯¼å…¥è¯„ä¼°å™¨
        from evaluation.benchmark import BenchmarkEvaluator
        
        # æ ¹æ®åœºæ™¯åˆ›å»ºè¯„ä¼°å™¨ï¼Œä½¿ç”¨mobile_enæ•°æ®
        if scenario == 1:
            evaluator = BenchmarkEvaluator(data_root="mobile_en", result_prefix="mobile")
        elif scenario == 2:
            evaluator = BenchmarkEvaluator(
                data_root="mobile_en",
                use_cache=scenario_info['use_cache'], 
                cache_source_dir=scenario_info['cache_source_dir'],
                result_prefix="mobile"
            )
        elif scenario == 3:
            evaluator = BenchmarkEvaluator(
                data_root="mobile_en",
                use_cache=scenario_info['use_cache'], 
                cache_source_dir=scenario_info['cache_source_dir'],
                result_prefix="mobile"
            )
        
        # è®¾ç½®è¿›åº¦å›è°ƒ
        evaluator.set_progress_callback(simple_progress)
        
        start_time = time.time()
        
        # è¿è¡Œè¯„ä¼°
        evaluator.run_evaluation(
            model_name=model_name,
            limit=limit,
            scenario=scenario,
            detector_model=scenario_info['detector_model'],
            use_ground_truth=scenario_info['use_ground_truth']
        )
        
        duration = time.time() - start_time
        
        logger.info(f"âœ… {model_name} åœºæ™¯{scenario} æµ‹è¯•å®Œæˆ - è€—æ—¶: {duration:.1f}ç§’")
        
        # æ ‡è®°ä»»åŠ¡å®Œæˆ
        progress_data = load_progress()
        mark_task_completed(model_name, scenario, progress_data)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ {model_name} åœºæ™¯{scenario} æµ‹è¯•å¤±è´¥: {str(e)}")
        return False
    finally:
        clear_gpu_memory()

def main():
    """ä¸»å‡½æ•°ï¼ˆå¹¶è¡Œç‰ˆï¼Œæ”¯æŒæ–­ç‚¹æ¢å¤ï¼‰"""
    # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°
    selected_model = os.environ.get('SELECTED_MODEL', 'google/gemini-2.5-flash')
    models = [selected_model]
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®è¿è¡Œåœºæ™¯
    if selected_model.startswith('google/gemini'):
        scenarios = [1, 2]  # Geminiæ¨¡å‹è¿è¡Œåœºæ™¯1å’Œ2
        max_workers = min(2, os.cpu_count())  # ä¸¤ä¸ªåœºæ™¯å¯ä»¥å¹¶è¡Œ
    else:
        scenarios = [1]  # Claudeå’ŒGPTåªè¿è¡Œåœºæ™¯1
        max_workers = 1  # å•åœºæ™¯å•è¿›ç¨‹
    
    limit = 650

    # æ£€æŸ¥ç¯å¢ƒ
    if not os.path.exists("config/models_config.yaml"):
        logger.error("âŒ æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®æ–‡ä»¶: config/models_config.yaml")
        return False
    
    # æ£€æŸ¥mobile_enæ•°æ®æ˜¯å¦å­˜åœ¨
    if not os.path.exists("mobile_en"):
        logger.error("âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®ç›®å½•: mobile_en")
        return False

    # åŠ è½½è¿›åº¦æ•°æ®
    progress_data = load_progress()
    
    # è¿‡æ»¤æ‰å·²å®Œæˆçš„ä»»åŠ¡
    all_tasks = [(m, s) for m in models for s in scenarios]
    pending_tasks = [(m, s) for m, s in all_tasks if not is_task_completed(m, s, progress_data)]
    
    total_tests = len(all_tasks)
    completed_tests = total_tests - len(pending_tasks)
    success_count = completed_tests

    if completed_tests > 0:
        logger.info(f"â­ï¸  å‘ç°å·²å®Œæˆçš„ä»»åŠ¡: {completed_tests}/{total_tests}")
        logger.info(f"ğŸ“ è·³è¿‡å·²å®Œæˆä»»åŠ¡ï¼Œç»§ç»­æ‰§è¡Œå‰©ä½™ {len(pending_tasks)} ä¸ªä»»åŠ¡")
    else:
        logger.info(f"ğŸš€ å¼€å§‹å…¨æ–°è¯„ä¼°ï¼Œå…± {total_tests} ä¸ªä»»åŠ¡")

    if not pending_tasks:
        logger.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
        return True

    logger.info(f"å°†æ‰§è¡Œ {len(pending_tasks)} ä¸ªå‰©ä½™ä»»åŠ¡")

    # æ–­ç‚¹ä¿æŠ¤ï¼šæ·»åŠ ç¡®è®¤æœºåˆ¶
    if len(pending_tasks) > 0:
        logger.info("âš ï¸  æ–­ç‚¹ä¿æŠ¤ï¼šå³å°†å¼€å§‹æ¨¡å‹è¯„ä¼°")
        logger.info(f"æ¨¡å‹: {pending_tasks[0][0]}")
        logger.info(f"åœºæ™¯: {pending_tasks[0][1]}")
        logger.info("å¦‚éœ€åœæ­¢ï¼Œè¯·åœ¨5ç§’å†…æŒ‰ Ctrl+C")
        try:
            import time
            time.sleep(5)
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œå®‰å…¨é€€å‡º")
            return False

    with ProcessPoolExecutor(max_workers=max_workers) as pool:
        future_map = {
            pool.submit(run_scenario_test, m, s, limit): (m, s)
            for m, s in pending_tasks
        }

        for future in as_completed(future_map):
            m, s = future_map[future]
            try:
                ok = future.result()
                if ok:
                    success_count += 1
                    logger.info(f"âœ… {m} åœºæ™¯{s} å®Œæˆ")
                else:
                    logger.error(f"âŒ {m} åœºæ™¯{s} å¤±è´¥")
            except Exception as e:
                logger.error(f"âŒ {m} åœºæ™¯{s} å¼‚å¸¸: {e}")

    final_success_count = success_count
    logger.info(f"ğŸ“Š è¯„ä¼°å®Œæˆç»Ÿè®¡:")
    logger.info(f"   æ€»ä»»åŠ¡æ•°: {total_tests}")
    logger.info(f"   å·²å®Œæˆ: {final_success_count}")
    logger.info(f"   æˆåŠŸç‡: {final_success_count/total_tests*100:.1f}%")
    
    return final_success_count == total_tests

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
EOF

# è¿è¡ŒPythonè¯„ä¼°è„šæœ¬
python run_online_evaluation.py

# æ£€æŸ¥è¯„ä¼°ç»“æœ
if [ $? -eq 0 ]; then
    echo "âœ… è¯„ä¼°æˆåŠŸå®Œæˆ $(date)" | tee -a $LOG_FILE
    echo "è¿›åº¦æ–‡ä»¶ä¿å­˜åœ¨: $PROGRESS_FILE" | tee -a $LOG_FILE
else
    echo "âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ $(date)" | tee -a $LOG_FILE
    echo "è¿›åº¦å·²ä¿å­˜ï¼Œå¯é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­æ‰§è¡Œ" | tee -a $LOG_FILE
fi

# è®°å½•å®Œæˆæ—¶é—´
echo "æµ‹è¯•å®Œæˆ $(date)" | tee -a $LOG_FILE

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f run_online_evaluation.py