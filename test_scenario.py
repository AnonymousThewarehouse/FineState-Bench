import sys
import os
import json
import logging
import time
import subprocess
from concurrent.futures import ProcessPoolExecutor, as_completed
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸  å»ºè®®å®‰è£… tqdm ä»¥æ˜¾ç¤ºè¿›åº¦æ¡: pip install tqdm")

sys.path.append(os.getcwd())

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

OFFLINE_SCENARIO2_MODELS = [
    "ShowUI-2B",
    "OS-Atlas-Base-7B"
]

AVAILABLE_DATASETS = [
    "desktop_en",
    "mobile_en", 
    "web_en"
]

SCENARIO2_CONFIGS = {
    "gpt4o": {
        "name": "ç»„ä»¶æ£€æµ‹å¢å¼º(GPT-4o)",
        "description": "ä½¿ç”¨GPT-4oç¼“å­˜çš„å¢å¼ºprompt",
        "detector_model": "openai/gpt-4o-2024-11-20",
        "use_ground_truth": False,
        "use_cache": True,
        "cache_source_dir": "openai/gpt-4o-2024-11-20_bygpt-4o-2024-11-20"
    },
    "gemini": {
        "name": "ç»„ä»¶æ£€æµ‹å¢å¼º(Gemini-2.5-Flash)",
        "description": "ä½¿ç”¨Gemini-2.5-Flashç¼“å­˜çš„å¢å¼ºprompt",
        "detector_model": "google/gemini-2.5-flash",
        "use_ground_truth": False,
        "use_cache": True,
        "cache_source_dir": "google/gemini-2.5-flash_bygemini-2.5-flash"
    }
}

def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜ - ä»…åœ¨GPU 6å’Œ7ä¸Š"""
    try:
        cleanup_code = """
import torch
import gc
import os

# å¤šæ¬¡åƒåœ¾å›æ”¶
for _ in range(3):
    gc.collect()
    
if torch.cuda.is_available():
    # æ¸…ç†æ‰€æœ‰å¯è§GPUçš„ç¼“å­˜
    for i in range(torch.cuda.device_count()):
        with torch.cuda.device(i):
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
    torch.cuda.synchronize()
    
# å¼ºåˆ¶é‡Šæ”¾æœªä½¿ç”¨çš„ç¼“å­˜æ± å†…å­˜
torch.cuda.empty_cache()
"""
        subprocess.run([
            'python', '-c', cleanup_code
        ], env={**os.environ, 'CUDA_VISIBLE_DEVICES': '6,7'}, 
        capture_output=True, timeout=30)
        logger.info("GPU 6,7 å†…å­˜å·²ç§¯ææ¸…ç†")
    except Exception as e:
        logger.warning(f"æ¸…ç†GPUå†…å­˜å¤±è´¥: {e}")

def check_model_completion(model_name: str, scenario: int, dataset: str) -> bool:
    result_dir = f"results/{dataset}_scenario{scenario}" if scenario == 2 else f"results/offline_scenario{scenario}"
    metrics_path = f"{result_dir}/{model_name}/{model_name}_metrics.json"
    
    if os.path.exists(metrics_path):
        logger.info(f"âœ… å‘ç°å·²å®Œæˆçš„ç»“æœ: {metrics_path}")
        return True
    else:
        logger.info(f"âŒ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶: {metrics_path}")
        return False

def list_existing_results(dataset: str, scenario: int) -> dict:
    result_dir = f"results/{dataset}_scenario{scenario}" if scenario == 2 else f"results/offline_scenario{scenario}"
    existing_results = {}
    
    logger.info(f"ğŸ” æ£€æŸ¥ç»“æœç›®å½•: {result_dir}")
    
    if os.path.exists(result_dir):
        for model_name in OFFLINE_SCENARIO2_MODELS:
            model_dir = f"{result_dir}/{model_name}"
            metrics_file = f"{model_dir}/{model_name}_metrics.json"
            
            if os.path.exists(metrics_file):
                try:
                    with open(metrics_file, 'r') as f:
                        data = json.load(f)

                    mod_time = os.path.getmtime(metrics_file)
                    mod_time_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(mod_time))
                    
                    existing_results[model_name] = {
                        'file_path': metrics_file,
                        'modification_time': mod_time_str,
                        'total_cases': data.get('total_cases', 'Unknown'),
                        'locate_success_rate': data.get('locate_success_rate', 0),
                        'interaction_success_rate': data.get('interaction_success_rate', 0)
                    }
                    logger.info(f"âœ… {model_name}: å·²å®Œæˆ (ä¿®æ”¹æ—¶é—´: {mod_time_str})")
                except Exception as e:
                    logger.warning(f"âš ï¸ {model_name}: ç»“æœæ–‡ä»¶å­˜åœ¨ä½†è¯»å–å¤±è´¥: {e}")
            else:
                logger.info(f"âŒ {model_name}: æœªå®Œæˆ")
    else:
        logger.info(f"ğŸ“ ç»“æœç›®å½•ä¸å­˜åœ¨ï¼Œå°†åˆ›å»º: {result_dir}")
    
    return existing_results

def read_metrics_file_with_dataset(model_name: str, scenario: int, dataset: str) -> dict:
    if scenario == 2:
        result_dir = f"results/{dataset}_scenario2"
    else:
        if dataset == "desktop_en":
            result_dir = f"results/offline_scenario{scenario}"
        elif dataset == "mobile_en":
            result_dir = "results/mobile"
        elif dataset == "web_en":
            result_dir = "results/web"
        else:
            result_dir = f"results/offline_scenario{scenario}"
    
    metrics_path = f"{result_dir}/{model_name}/{model_name}_metrics.json"
    if os.path.exists(metrics_path):
        with open(metrics_path, 'r') as f:
            return json.load(f)
    return {}

def wait_for_gpu_availability(gpu_ids=None):
    if gpu_ids is None:
        gpu_ids = [6, 7]
    
    gpu_list = ','.join(map(str, gpu_ids))
    logger.info(f"ğŸ” æ£€æŸ¥GPU {gpu_list}çš„å¯ç”¨æ€§...")
    
    while True:
        try:
            result = subprocess.run([
                'nvidia-smi', '--query-gpu=memory.used,memory.total,memory.free',
                '--format=csv,noheader,nounits', f'--id={gpu_list}'
            ], capture_output=True, text=True, check=True)
            
            lines = result.stdout.strip().split('\n')
            gpus_available = 0
            
            for i, line in enumerate(lines):
                if line.strip():
                    parts = line.split(', ')
                    used_mb = int(parts[0])
                    total_mb = int(parts[1])
                    free_mb = int(parts[2])
                    
                    usage_percent = (used_mb / total_mb) * 100
                    free_gb = free_mb / 1024
                    
                    gpu_id = gpu_ids[i]
                    logger.info(f"GPU {gpu_id}: ä½¿ç”¨ç‡ {usage_percent:.1f}%, ç©ºé—²å†…å­˜ {free_gb:.1f}GB")

                    if usage_percent < 70 and free_gb > 8:
                        gpus_available += 1
            
            if gpus_available >= len(gpu_ids):
                logger.info(f"âœ… GPU {gpu_list}éƒ½å¯ç”¨ï¼Œå¼€å§‹æµ‹è¯•")
                break
            else:
                logger.info(f"â³ ç­‰å¾…GPUå¯ç”¨ (å½“å‰å¯ç”¨: {gpus_available}/{len(gpu_ids)})ï¼Œ60ç§’åé‡è¯•...")
                time.sleep(60)
                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥GPUçŠ¶æ€å¤±è´¥: {e}ï¼Œ30ç§’åé‡è¯•...")
            time.sleep(30)

def select_detector_model():
    print("\n" + "="*50)
    print("è¯·é€‰æ‹©è¾¹æ¡†é¢„æµ‹æ¨¡å‹:")
    print("  1. GPT-4o (openai/gpt-4o-2024-11-20)")
    print("  2. Gemini-2.5-Flash (google/gemini-2.5-flash)")
    print("="*50)
    
    while True:
        try:
            choice = int(input("è¯·è¾“å…¥æ¨¡å‹ç¼–å· (1-2): "))
            if choice == 1:
                return "gpt4o"
            elif choice == 2:
                return "gemini"
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-2")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

def select_dataset():
    print("\n" + "="*50)
    print("è¯·é€‰æ‹©æ•°æ®é›†:")
    print("  1. Desktop (desktop_en) - 628ä¸ªæ ·æœ¬")
    print("  2. Mobile (mobile_en) - 648ä¸ªæ ·æœ¬")
    print("  3. Web (web_en) - 697ä¸ªæ ·æœ¬")
    print("="*50)
    
    while True:
        try:
            choice = int(input("è¯·è¾“å…¥æ•°æ®é›†ç¼–å· (1-3): "))
            if choice == 1:
                return "desktop_en", 628
            elif choice == 2:
                return "mobile_en", 648
            elif choice == 3:
                return "web_en", 697
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-3")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

def run_scenario_test_with_dataset(model_name: str, scenario: int, dataset: str, limit: int = 10, gpu_id: int = None, detector_key: str = "gpt4o") -> bool:
    scenario_info = SCENARIO2_CONFIGS[detector_key] 

    if gpu_id is None:
        gpu_ids = [6, 7]
        cuda_visible_devices = '6,7'
    else:
        gpu_ids = [gpu_id]
        cuda_visible_devices = str(gpu_id)

    non_interactive = os.environ.get('TEST_NON_INTERACTIVE', '0') == '1'
    
    if not non_interactive:
        logger.info("="*60)
        logger.info(f"ğŸš€ å¼€å§‹æµ‹è¯•")
        logger.info(f"æ¨¡å‹: {model_name}")
        logger.info(f"æ•°æ®é›†: {dataset}")
        logger.info(f"åœºæ™¯: {scenario} ({scenario_info['name']})")
        logger.info(f"æè¿°: {scenario_info['description']}")
        logger.info(f"æµ‹è¯•ç”¨ä¾‹æ•°: {limit}")
        logger.info(f"ğŸ¯ æŒ‡å®šGPU: {cuda_visible_devices}")
        logger.info("="*60)

    wait_for_gpu_availability(gpu_ids)
    
    try:
        old_cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')
        old_tokenizers_parallelism = os.environ.get('TOKENIZERS_PARALLELISM')
        old_omp_threads = os.environ.get('OMP_NUM_THREADS')
        
        os.environ['CUDA_VISIBLE_DEVICES'] = cuda_visible_devices
        os.environ['TEST_NON_INTERACTIVE'] = '1'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['OMP_NUM_THREADS'] = '4'
        os.environ['MKL_NUM_THREADS'] = '4'
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'

        test_code = f'''
#!/usr/bin/env python3
import os
import sys
import logging
import time
import json
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

# æ€§èƒ½ä¼˜åŒ–ç¯å¢ƒå˜é‡è®¾ç½®
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å…tokenizerè­¦å‘Š
os.environ['OMP_NUM_THREADS'] = '4'  # é™åˆ¶OpenMPçº¿ç¨‹æ•°
os.environ['MKL_NUM_THREADS'] = '4'  # é™åˆ¶MKLçº¿ç¨‹æ•°
os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0;8.6'  # é’ˆå¯¹ç°ä»£GPUä¼˜åŒ–
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # å¼‚æ­¥CUDAå¯åŠ¨

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.getcwd())

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜ - å¢å¼ºç‰ˆ"""
    try:
        import torch
        import gc
        
        # å¤šæ¬¡åƒåœ¾å›æ”¶
        for _ in range(3):
            gc.collect()
            
        if torch.cuda.is_available():
            # æ¸…ç†å½“å‰è®¾å¤‡ç¼“å­˜
            torch.cuda.empty_cache()
            # æ¸…ç†IPCç¼“å­˜
            torch.cuda.ipc_collect()
            # åŒæ­¥æ‰€æœ‰æµ
            torch.cuda.synchronize()
            logger.info("GPUå†…å­˜å·²æ·±åº¦æ¸…ç†")
    except Exception as e:
        logger.warning(f"æ¸…ç†GPUå†…å­˜å¤±è´¥: {{e}}")

def run_scenario_test(model_name: str, scenario: int, dataset: str, limit: int) -> bool:
    """è¿è¡ŒæŒ‡å®šåœºæ™¯çš„æµ‹è¯•"""
    scenario_config = {{
        "name": "{scenario_info['name']}",
        "description": "{scenario_info['description']}",
        "detector_model": "{scenario_info['detector_model']}",
        "use_ground_truth": {scenario_info['use_ground_truth']},
        "use_cache": {scenario_info['use_cache']},
        "cache_source_dir": "{scenario_info['cache_source_dir']}"
    }}
    
    logger.info("="*60)
    logger.info(f"ğŸš€ å¼€å§‹æµ‹è¯•")
    logger.info(f"æ¨¡å‹: {{model_name}}")
    logger.info(f"æ•°æ®é›†: {{dataset}}")
    logger.info(f"åœºæ™¯: {{scenario}} ({{scenario_config['name']}})")  
    logger.info(f"æè¿°: {{scenario_config['description']}}")
    logger.info(f"æµ‹è¯•ç”¨ä¾‹æ•°: {{limit}}")
    logger.info("="*60)
    
    # ç®€å•è¿›åº¦å›è°ƒ
    def simple_progress(current: int, total: int, message: str = ""):
        percentage = (current / total) * 100 if total > 0 else 0
        logger.info(f"è¿›åº¦: {{current}}/{{total}} ({{percentage:.1f}}%) - {{message}}")
    
    try:
        # å¯¼å…¥è¯„ä¼°å™¨
        from evaluation.benchmark import BenchmarkEvaluator
        
        # åˆ›å»ºè¯„ä¼°å™¨ï¼Œä½¿ç”¨æŒ‡å®šæ•°æ®é›† - æ·»åŠ æ€§èƒ½ä¼˜åŒ–å‚æ•°
        evaluator = BenchmarkEvaluator(
            data_root=dataset,
            use_cache=scenario_config['use_cache'], 
            cache_source_dir=scenario_config['cache_source_dir']
        )
        
        # è®¾ç½®æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(evaluator, 'set_batch_processing'):
            evaluator.set_batch_processing(True)
        if hasattr(evaluator, 'set_memory_optimization'):
            evaluator.set_memory_optimization(True)
        
        # è®¾ç½®è¿›åº¦å›è°ƒ
        evaluator.set_progress_callback(simple_progress)
        
        start_time = time.time()
        
        # è¿è¡Œè¯„ä¼°
        evaluator.run_evaluation(
            model_name=model_name,
            limit=limit,
            scenario=scenario,
            detector_model=scenario_config['detector_model'],
            use_ground_truth=scenario_config['use_ground_truth']
        )
        
        duration = time.time() - start_time
        
        logger.info(f"âœ… {{model_name}} on {{dataset}} åœºæ™¯{{scenario}} æµ‹è¯•å®Œæˆ - è€—æ—¶: {{duration:.1f}}ç§’")
        return True
        
    except Exception as e:
        logger.error(f"âŒ {{model_name}} on {{dataset}} åœºæ™¯{{scenario}} æµ‹è¯•å¤±è´¥: {{str(e)}}")
        return False
    finally:
        clear_gpu_memory()

def main():
    """ä¸»å‡½æ•°"""
    model_name = "{model_name}"
    dataset = "{dataset}"
    scenario = {scenario}
    limit = {limit}
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not os.path.exists("config/models_config.yaml"):
        logger.error("âŒ æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®æ–‡ä»¶: config/models_config.yaml")
        return False
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dataset):
        logger.error(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®ç›®å½•: {{dataset}}")
        return False
    
    # è¿è¡Œæµ‹è¯•
    success = run_scenario_test(model_name, scenario, dataset, limit)
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
'''

        script_filename = f"run_test_{model_name.replace('-', '_')}_{dataset}_{scenario}.py"
        with open(script_filename, 'w', encoding='utf-8') as f:
            f.write(test_code)
        
        logger.info(f"ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        logger.info(f"ğŸ“¦ æ¨¡å‹: {model_name}")
        logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset}")
        logger.info(f"ğŸ¯ åœºæ™¯: {scenario} ({scenario_info['name']})")
        logger.info(f"ğŸ“Š æµ‹è¯•ç”¨ä¾‹æ•°: {limit}")
        logger.info(f"ğŸ”§ ä½¿ç”¨GPU: {cuda_visible_devices} (ä¼˜åŒ–æ¨¡å¼)")
        logger.info(f"âš¡ æ€§èƒ½ä¼˜åŒ–: å¯ç”¨ç¼“å­˜, å¤šçº¿ç¨‹é™åˆ¶, å†…å­˜ä¼˜åŒ–")
        
        start_time = time.time()

        result = subprocess.run(
            ['python', script_filename],
            capture_output=True,
            text=True,
            timeout=None
        )
        
        duration = time.time() - start_time

        if os.path.exists(script_filename):
            os.remove(script_filename)
        
        if result.returncode == 0:
            metrics = read_metrics_file_with_dataset(model_name, scenario, dataset)
            
            if not non_interactive:
                logger.info("\n" + "="*60)
                logger.info("ğŸ¯ æµ‹è¯•å®Œæˆ!")
                logger.info(f"â±ï¸  è€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
                
                if metrics:
                    logger.info("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
                    logger.info(f"  æ€»æµ‹è¯•ç”¨ä¾‹: {metrics.get('total_cases', 0)}")
                    logger.info(f"  å®šä½æˆåŠŸç‡: {metrics.get('locate_success_rate', 0):.1%}")
                    logger.info(f"  äº¤äº’æˆåŠŸç‡: {metrics.get('interaction_success_rate', 0):.1%}")
                    logger.info(f"  å®šä½çŠ¶æ€æ„ŸçŸ¥ç‡: {metrics.get('state_awareness_rate_locate', 0):.1%}")
                    logger.info(f"  äº¤äº’çŠ¶æ€æ„ŸçŸ¥ç‡: {metrics.get('state_awareness_rate_interaction', 0):.1%}")

                    locate_rate = metrics.get('locate_success_rate', 0)
                    interact_rate = metrics.get('interaction_success_rate', 0)
                    total_success_rate = (locate_rate + interact_rate) / 2
                    
                    logger.info(f"\nğŸ¯ æ€»ä½“æˆåŠŸç‡: {total_success_rate:.1%}")
                    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset}")
                    logger.info(f"\nâœ… åœºæ™¯{scenario}æµ‹è¯•å®Œæˆï¼Œå¢å¼ºæ•ˆæœå·²ä½“ç°åœ¨æˆåŠŸç‡ä¸­")
                else:
                    logger.warning("âš ï¸  æ— æ³•è¯»å–metricsæ–‡ä»¶ï¼Œä½†æµ‹è¯•å·²å®Œæˆ")
                
                logger.info("="*60)
            else:
                print(f"âœ… {model_name} on {dataset} åœºæ™¯{scenario} æµ‹è¯•å®Œæˆ - è€—æ—¶: {duration:.1f}ç§’")
            return True
        else:
            error_msg = result.stderr if result.stderr else result.stdout
            if not non_interactive:
                logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {error_msg}")
            else:
                print(f"âŒ {model_name} on {dataset} åœºæ™¯{scenario} æµ‹è¯•å¤±è´¥: {error_msg}")
            return False
        
    except Exception as e:
        if not non_interactive:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        else:
            print(f"âŒ {model_name} on {dataset} åœºæ™¯{scenario} æµ‹è¯•å¤±è´¥: {str(e)}")
        return False
    finally:
        if old_cuda_visible_devices is not None:
            os.environ['CUDA_VISIBLE_DEVICES'] = old_cuda_visible_devices
        else:
            os.environ.pop('CUDA_VISIBLE_DEVICES', None)
            
        if old_tokenizers_parallelism is not None:
            os.environ['TOKENIZERS_PARALLELISM'] = old_tokenizers_parallelism
        else:
            os.environ.pop('TOKENIZERS_PARALLELISM', None)
            
        if old_omp_threads is not None:
            os.environ['OMP_NUM_THREADS'] = old_omp_threads
        else:
            os.environ.pop('OMP_NUM_THREADS', None)
            
        os.environ.pop('MKL_NUM_THREADS', None)
        os.environ.pop('CUDA_LAUNCH_BLOCKING', None)
        
        if 'script_filename' in locals() and os.path.exists(script_filename):
            os.remove(script_filename)
            
        clear_gpu_memory()

def run_single_model_test(model_name: str, dataset: str, limit: int, gpu_id: int, detector_key: str = "gpt4o") -> bool:
    logger.info(f"ğŸ”„ å¼€å§‹æµ‹è¯•: {model_name} on {dataset} (GPU {gpu_id}) ä½¿ç”¨ {SCENARIO2_CONFIGS[detector_key]['name']}")
    success = run_scenario_test_with_dataset(model_name, 2, dataset, limit, gpu_id, detector_key)
    
    if success:
        logger.info(f"âœ… {model_name} on {dataset} (GPU {gpu_id}) æµ‹è¯•å®Œæˆ")
    else:
        logger.error(f"âŒ {model_name} on {dataset} (GPU {gpu_id}) æµ‹è¯•å¤±è´¥")
    
    return success

def run_offline_scenario2_on_dataset(dataset: str, limit: int, detector_key: str = "gpt4o"):
    print("\n" + "="*60)
    print(f"ğŸš€ ç¦»çº¿åœºæ™¯2æµ‹è¯• - {dataset}")
    print("ğŸ“‹ æµ‹è¯•é…ç½®:")
    print(f"  æ¨¡å‹: {', '.join(OFFLINE_SCENARIO2_MODELS)}")
    print(f"  æ•°æ®é›†: {dataset}")
    print(f"  åœºæ™¯: 2 ({SCENARIO2_CONFIGS[detector_key]['name']})")
    print(f"  æ£€æµ‹å™¨: {SCENARIO2_CONFIGS[detector_key]['detector_model']}")
    print(f"  æ ·æœ¬æ•°: {limit}")
    print(f"  å¹¶è¡Œæ‰§è¡Œ: ShowUI-2B(GPU 6), OS-Atlas-7B(GPU 7)")
    print("="*60)
    
    print("\nğŸ” æ£€æŸ¥ç°æœ‰ç»“æœæ–‡ä»¶...")
    existing_results = list_existing_results(dataset, 2)
    
    if existing_results:
        print(f"\nğŸ“Š å‘ç°å·²å®Œæˆçš„æ¨¡å‹ç»“æœ:")
        for model_name, info in existing_results.items():
            print(f"  âœ… {model_name}: å®Œæˆæ—¶é—´ {info['modification_time']}")
            print(f"     å®šä½æˆåŠŸç‡: {info['locate_success_rate']:.1%}, äº¤äº’æˆåŠŸç‡: {info['interaction_success_rate']:.1%}")
        
        print(f"\nâš ï¸  æ–­ç‚¹ä¿æŠ¤: å‘ç° {len(existing_results)} ä¸ªå·²å®Œæˆçš„æ¨¡å‹")
        rerun_choice = input("æ˜¯å¦é‡æ–°è¿è¡Œå·²å®Œæˆçš„æ¨¡å‹? (y/N): ").strip().lower()
        
        if rerun_choice not in ['y', 'yes']:
            print("âœ… å°†è·³è¿‡å·²å®Œæˆçš„æ¨¡å‹ï¼Œåªè¿è¡Œæœªå®Œæˆçš„æ¨¡å‹")
        else:
            print("ğŸ”„ å°†é‡æ–°è¿è¡Œæ‰€æœ‰æ¨¡å‹")
            existing_results = {} 
    else:
        print("ğŸ“­ æœªå‘ç°å·²å®Œæˆçš„ç»“æœï¼Œå°†è¿è¡Œæ‰€æœ‰æ¨¡å‹")

    print(f"\n{'='*60}")
    if existing_results:
        remaining_models = [m for m in OFFLINE_SCENARIO2_MODELS if m not in existing_results]
        if remaining_models:
            print(f"ğŸ“‹ å¾…è¿è¡Œæ¨¡å‹: {', '.join(remaining_models)}")
            confirm = input("ç¡®è®¤å¼€å§‹æµ‹è¯•æœªå®Œæˆçš„æ¨¡å‹? (y/N): ").strip().lower()
        else:
            print("ğŸ‰ æ‰€æœ‰æ¨¡å‹éƒ½å·²å®Œæˆï¼")
            return
    else:
        confirm = input("ç¡®è®¤å¼€å§‹æµ‹è¯•æ‰€æœ‰æ¨¡å‹? (y/N): ").strip().lower()
    
    if confirm not in ['y', 'yes']:
        print("âŒ æµ‹è¯•å·²å–æ¶ˆ")
        return

    if not os.path.exists("config/models_config.yaml"):
        logger.error("âŒ æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®æ–‡ä»¶: config/models_config.yaml")
        return

    if not os.path.exists(dataset):
        logger.error(f"âŒ æ‰¾ä¸åˆ°æ•°æ®é›†ç›®å½•: {dataset}")
        return
    
    remaining_models = [m for m in OFFLINE_SCENARIO2_MODELS if m not in existing_results]
    
    if not remaining_models:
        logger.info("ğŸ‰ æ‰€æœ‰æ¨¡å‹éƒ½å·²å®Œæˆï¼Œæ— éœ€è¿è¡Œ!")
        return

    required_gpus = []
    for model_name in remaining_models:
        if model_name == "ShowUI-2B":
            required_gpus.append(6)
        elif model_name == "OS-Atlas-Base-7B":
            required_gpus.append(7)
    
    logger.info(f"ğŸ¯ éœ€è¦ä½¿ç”¨çš„GPU: {required_gpus}")
    wait_for_gpu_availability(required_gpus)
    
    total_tests = len(remaining_models)
    total_original = len(OFFLINE_SCENARIO2_MODELS)
    completed_already = len(existing_results)
    logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæµ‹è¯•ï¼Œå…± {total_tests} ä¸ªå¾…è¿è¡Œæ¨¡å‹ (å·²å®Œæˆ: {completed_already}/{total_original})")

    tasks = []
    for model_name in OFFLINE_SCENARIO2_MODELS:
        if model_name in remaining_models:
            gpu_id = 6 if model_name == "ShowUI-2B" else 7
            tasks.append((model_name, dataset, limit, gpu_id))
            logger.info(f"ğŸ“‹ å‡†å¤‡ä»»åŠ¡: {model_name} â†’ GPU {gpu_id}")
    
    completed_tests = completed_already  
    failed_tests = 0

    with ProcessPoolExecutor(max_workers=2) as executor:
        future_to_task = {
            executor.submit(run_single_model_test, model_name, dataset, limit, gpu_id, detector_key): (model_name, gpu_id)
            for model_name, dataset, limit, gpu_id in tasks
        }
        
        for future in as_completed(future_to_task):
            model_name, gpu_id = future_to_task[future]
            try:
                success = future.result()
                if success:
                    completed_tests += 1
                else:
                    failed_tests += 1
            except Exception as e:
                logger.error(f"âŒ {model_name} (GPU {gpu_id}) æ‰§è¡Œå¼‚å¸¸: {e}")
                failed_tests += 1

    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“Š å¹¶è¡Œæµ‹è¯•å®Œæˆç»Ÿè®¡ - {dataset}:")
    logger.info(f"  æ€»æ¨¡å‹æ•°: {total_original}")
    logger.info(f"  å·²å®Œæˆ: {completed_tests}")
    logger.info(f"  æœ¬æ¬¡è¿è¡Œ: {len(tasks)}")
    logger.info(f"  å¤±è´¥æµ‹è¯•: {failed_tests}")
    logger.info(f"  æ€»ä½“æˆåŠŸç‡: {completed_tests/total_original*100:.1f}%")

    logger.info(f"\nğŸ“ ç»“æœä¿å­˜è·¯å¾„:")
    result_base_dir = f"results/{dataset}_scenario2"
    for model_name in OFFLINE_SCENARIO2_MODELS:
        model_dir = f"{result_base_dir}/{model_name}"
        metrics_file = f"{model_dir}/{model_name}_metrics.json"
        if os.path.exists(metrics_file):
            logger.info(f"  âœ… {model_name}: {metrics_file}")
        else:
            logger.info(f"  âŒ {model_name}: {metrics_file} (æœªå®Œæˆ)")
    
    logger.info(f"{'='*80}")
    
    if completed_tests == total_original:
        logger.info(f"ğŸ‰ {dataset} æ‰€æœ‰æ¨¡å‹æµ‹è¯•æˆåŠŸå®Œæˆ!")
    elif failed_tests > 0:
        logger.warning(f"âš ï¸ {dataset} æœ‰ {failed_tests} ä¸ªæµ‹è¯•å¤±è´¥")
    else:
        logger.info(f"âœ… {dataset} æœ¬æ¬¡è¿è¡Œçš„ {len(tasks)} ä¸ªæ¨¡å‹æµ‹è¯•å®Œæˆ!")

def main():

    print("=" * 60)
    print("ğŸ¯ SpiderBench ç¦»çº¿åœºæ™¯2æµ‹è¯•å·¥å…·")
    print("ğŸ“¦ æµ‹è¯•æ¨¡å‹: ShowUI-2B, OS-Atlas-7B")
    print("ğŸ¯ æµ‹è¯•åœºæ™¯: 2 (ç»„ä»¶æ£€æµ‹å¢å¼º)")
    print("ğŸ”§ æŒ‡å®šGPU: 6,7")
    print("=" * 60)
    
    if not TQDM_AVAILABLE:
        print("ğŸ’¡ æç¤º: å®‰è£… tqdm åº“å¯ä»¥æ˜¾ç¤ºæ›´å¥½çš„è¿›åº¦æ¡:")
        print("   pip install tqdm")
        print()
    
    if not os.path.exists("config/models_config.yaml"):
        logger.error("âŒ æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®æ–‡ä»¶: config/models_config.yaml")
        return
    
    try:
        detector_key = select_detector_model()
        
        dataset, limit = select_dataset()
        
        run_offline_scenario2_on_dataset(dataset, limit, detector_key)
            
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    main()